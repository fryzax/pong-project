# üèì Projet PPO Pong - Apprentissage par Renforcement

Ce projet impl√©mente un agent PPO (Proximal Policy Optimization) pour jouer au jeu Atari Pong, avec des outils complets de fine-tuning et d'analyse des performances.

## üìã Structure du Projet

```
.
‚îú‚îÄ‚îÄ pong.py                     # Script d'entra√Ænement principal
‚îú‚îÄ‚îÄ finetune_pong.py           # Script de fine-tuning avec tracking
‚îú‚îÄ‚îÄ play_pong.py               # Jouer contre l'agent (mode auto)
‚îú‚îÄ‚îÄ play_interactive_pong.py   # Jouer contre l'agent (mode interactif)
‚îú‚îÄ‚îÄ analyze_performance.py     # Analyser et comparer les performances
‚îú‚îÄ‚îÄ ppo_pong_*/                # Dossiers des mod√®les entra√Æn√©s
‚îÇ   ‚îú‚îÄ‚îÄ best_model.pth        # Meilleur mod√®le
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint_*.pth      # Checkpoints r√©guliers
‚îÇ   ‚îú‚îÄ‚îÄ metrics.json          # M√©triques d'entra√Ænement
‚îÇ   ‚îú‚îÄ‚îÄ config.json           # Configuration utilis√©e
‚îÇ   ‚îî‚îÄ‚îÄ tensorboard/          # Logs TensorBoard
‚îî‚îÄ‚îÄ README.md                  # Ce fichier
```

## üöÄ Installation

```bash
# Installer les d√©pendances
pip install torch gymnasium ale-py scipy pygame tensorboard

# Cloner le repo
git clone https://github.com/fryzax/pong-project.git
cd pong-project
```

## üéØ Utilisation

### 1. Entra√Ænement Initial

```bash
# Entra√Æner un nouvel agent
python pong.py
```

Param√®tres configurables dans le code :
- `n_episodes`: Nombre d'√©pisodes (d√©faut: 1000)
- `learning_rate`: Taux d'apprentissage (d√©faut: 2.5e-4)
- `gamma`: Facteur de discount (d√©faut: 0.99)
- `update_every`: Fr√©quence de mise √† jour (d√©faut: 2048)

### 2. Fine-tuning

```bash
# Fine-tuner depuis le meilleur mod√®le (auto-d√©tection)
python finetune_pong.py --episodes 500

# Fine-tuner depuis un mod√®le sp√©cifique
python finetune_pong.py --model ppo_pong_20251021_110351/best_model.pth --episodes 500

# Param√®tres personnalis√©s
python finetune_pong.py --episodes 500 --lr 1e-4 --gamma 0.99 --clip 0.2 --plot
```

**Options disponibles:**
- `--model`: Chemin vers le mod√®le de base (auto-d√©tecte le plus r√©cent si non sp√©cifi√©)
- `--episodes`: Nombre d'√©pisodes de fine-tuning (d√©faut: 500)
- `--lr`: Learning rate (d√©faut: 1e-4)
- `--gamma`: Facteur de discount (d√©faut: 0.99)
- `--clip`: Epsilon de clipping PPO (d√©faut: 0.2)
- `--plot`: G√©n√©rer les graphiques √† la fin

**Suivi avec TensorBoard:**
```bash
# Lancer TensorBoard pour voir l'entra√Ænement en temps r√©el
tensorboard --logdir=ppo_pong_finetuned_XXXXXXXX_XXXXXX/tensorboard
```

### 3. Jouer contre l'Agent

#### Mode Automatique (regarder jouer)
```bash
python play_pong.py --games 3
```

#### Mode Interactif (vous jouez!)
```bash
python play_interactive_pong.py --games 3
```

**Contr√¥les:**
- ‚¨ÜÔ∏è Fl√®che HAUT : Monter la raquette
- ‚¨áÔ∏è Fl√®che BAS : Descendre la raquette
- ESC ou Q : Quitter

### 4. Analyser les Performances

```bash
# Comparer tous les mod√®les entra√Æn√©s
python analyze_performance.py --plot --report

# Comparer des mod√®les sp√©cifiques
python analyze_performance.py --models ppo_pong_A ppo_pong_B --plot

# Seulement g√©n√©rer le rapport texte
python analyze_performance.py --report

# Seulement g√©n√©rer les graphiques
python analyze_performance.py --plot --output comparison.png
```

**Sorties:**
- Tableau comparatif dans le terminal
- Graphiques de comparaison (si `--plot`)
- Rapport d√©taill√© en texte (si `--report`)

## üìä Suivi des Performances

### M√©triques Track√©es

Le syst√®me de fine-tuning enregistre automatiquement :

**Par √©pisode:**
- R√©compense totale
- Longueur de l'√©pisode
- Step global
- Timestamp

**Par mise √† jour:**
- Loss totale
- Policy loss
- Value loss
- Entropie
- Clip fraction (taux de clipping PPO)

### Fichiers G√©n√©r√©s

Chaque entra√Ænement cr√©e un dossier `ppo_pong_[finetuned_]YYYYMMDD_HHMMSS/` contenant :

1. **metrics.json**: Toutes les m√©triques au format JSON
   ```json
   {
     "episodes": [
       {"episode": 0, "reward": -21.0, "length": 1234, ...},
       ...
     ],
     "updates": [
       {"episode": 10, "total_loss": 0.123, ...},
       ...
     ]
   }
   ```

2. **config.json**: Configuration d'entra√Ænement
   ```json
   {
     "learning_rate": 0.0001,
     "gamma": 0.99,
     "clip_epsilon": 0.2,
     "base_model": "ppo_pong_*/best_model.pth",
     ...
   }
   ```

3. **tensorboard/**: Logs TensorBoard pour visualisation interactive

4. **best_model.pth**: Meilleur mod√®le (moyenne des 100 derniers √©pisodes)

5. **checkpoint_epXXX.pth**: Checkpoints r√©guliers avec √©tat complet

## üìà Visualisation avec TensorBoard

TensorBoard offre une visualisation en temps r√©el :

```bash
tensorboard --logdir=ppo_pong_finetuned_XXXXXXXX_XXXXXX/tensorboard
```

Puis ouvrir http://localhost:6006 dans votre navigateur.

**M√©triques disponibles:**
- Episode/Reward
- Episode/Length
- Episode/Global_Step
- Loss/Total
- Loss/Policy
- Loss/Value
- Loss/Entropy
- PPO/Clip_Fraction

## üéì Architecture du R√©seau

**CNN Policy Network:**
- Conv1: 32 filtres (8x8, stride=4) + ReLU
- Conv2: 64 filtres (4x4, stride=2) + ReLU
- Conv3: 64 filtres (3x3, stride=1) + ReLU
- FC1: 512 neurones + ReLU
- Actor head: Softmax sur actions
- Critic head: Estimation de valeur

**Preprocessing:**
- Conversion en niveaux de gris
- Resize 84x84
- Stack de 4 frames pour information temporelle

## ‚öôÔ∏è Hyperparam√®tres PPO

### Entra√Ænement Initial
- Learning rate: 2.5e-4
- Gamma: 0.99
- GAE Lambda: 0.95
- Clip epsilon: 0.2
- Epochs: 4
- Batch size: 256

### Fine-tuning (Recommand√©)
- Learning rate: 1e-4 (plus faible)
- Gamma: 0.99
- Clip epsilon: 0.2
- Batch size: 256

## üìä R√©sultats Attendus

**Apr√®s entra√Ænement initial (~1000 √©pisodes):**
- R√©compense moyenne: -5 √† +5
- Performance stable avec quelques victoires

**Apr√®s fine-tuning (~500 √©pisodes suppl√©mentaires):**
- R√©compense moyenne: +5 √† +15
- Am√©lioration de la consistance
- Meilleure anticipation

## üõ†Ô∏è Troubleshooting

### Erreur "No module named X"
```bash
pip install torch gymnasium ale-py scipy pygame tensorboard
```

### Fen√™tre de jeu ne s'affiche pas
- V√©rifier que pygame est install√©
- Essayer `play_pong.py` (mode demo) au lieu d'interactif

### Performance faible apr√®s fine-tuning
- R√©duire le learning rate: `--lr 5e-5`
- Augmenter le nombre d'√©pisodes: `--episodes 1000`
- V√©rifier que le bon mod√®le de base est charg√©

### TensorBoard ne d√©marre pas
```bash
pip install --upgrade tensorboard
tensorboard --logdir=<path> --host=localhost
```

## üìù Exemple de Workflow Complet

```bash
# 1. Entra√Ænement initial
python pong.py

# 2. V√©rifier les performances
python play_pong.py --games 5

# 3. Fine-tuner pour am√©liorer
python finetune_pong.py --episodes 500 --lr 1e-4 --plot

# 4. Suivre en temps r√©el
tensorboard --logdir=ppo_pong_finetuned_*/tensorboard

# 5. Comparer les mod√®les
python analyze_performance.py --plot --report

# 6. Jouer contre le meilleur mod√®le
python play_interactive_pong.py --games 3
```

## üéØ Objectifs de Performance

| M√©trique | Initial | Apr√®s Fine-tuning | Expert |
|----------|---------|-------------------|--------|
| R√©compense moyenne | -10 √† 0 | +5 √† +10 | +15 √† +21 |
| Taux de victoire | 20-40% | 60-80% | 90%+ |
| Longueur moyenne | 1000-2000 | 800-1500 | 500-1000 |

## ü§ù Contribution

Pour contribuer au projet :
1. Fork le repository
2. Cr√©er une branche (`git checkout -b feature/am√©lioration`)
3. Commit les changements (`git commit -am 'Ajout fonctionnalit√©'`)
4. Push la branche (`git push origin feature/am√©lioration`)
5. Ouvrir une Pull Request

## üìÑ Licence

MIT License - Voir LICENSE pour plus de d√©tails

## üë• Auteurs

- [@fryzax](https://github.com/fryzax)

## üôè Remerciements

- OpenAI Gym/Gymnasium pour l'environnement
- Stable Baselines3 pour l'inspiration
- PyTorch pour le framework deep learning
