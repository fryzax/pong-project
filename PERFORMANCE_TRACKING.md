# ğŸ“Š SystÃ¨me de Suivi des Performances - Guide Complet

## Vue d'ensemble

Le systÃ¨me de suivi des performances pour le projet PPO Pong offre un tracking complet et automatique de toutes les mÃ©triques d'entraÃ®nement, avec plusieurs outils d'analyse et de visualisation.

## ğŸ¯ FonctionnalitÃ©s Principales

### 1. Tracking Automatique

**MÃ©triques par Ã‰pisode:**
- âœ… RÃ©compense totale de l'Ã©pisode
- âœ… Longueur de l'Ã©pisode (nombre de frames)
- âœ… Step global (compteur cumulatif)
- âœ… Timestamp prÃ©cis

**MÃ©triques par Mise Ã  Jour:**
- âœ… Loss totale (policy + value + entropy)
- âœ… Policy loss (loss de la politique)
- âœ… Value loss (loss du critique)
- âœ… Entropie (exploration)
- âœ… Clip fraction (% d'actions clippÃ©es par PPO)

### 2. Formats de Sauvegarde

#### A. JSON (metrics.json)
Structure complÃ¨te et lisible :
```json
{
  "episodes": [
    {
      "episode": 0,
      "reward": -21.0,
      "length": 1234,
      "global_step": 1234,
      "timestamp": "2025-10-21T15:46:22.123456"
    },
    ...
  ],
  "updates": [
    {
      "episode": 10,
      "total_loss": 0.123,
      "policy_loss": 0.045,
      "value_loss": 0.067,
      "entropy": 1.234,
      "clip_fraction": 0.12,
      "timestamp": "2025-10-21T15:46:23.123456"
    },
    ...
  ]
}
```

#### B. TensorBoard
Visualisation interactive en temps rÃ©el :
- Graphiques scalaires interactifs
- Comparaison multi-run
- Filtrage et lissage
- Export des donnÃ©es

#### C. Configuration (config.json)
Sauvegarde tous les hyperparamÃ¨tres :
```json
{
  "base_model": "ppo_pong_20251021_110351/best_model.pth",
  "n_episodes": 500,
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "clip_epsilon": 0.2,
  "gae_lambda": 0.95,
  "batch_size": 256,
  "epochs": 4,
  "timestamp": "20251021_154622",
  "device": "cpu"
}
```

## ğŸ“ˆ Utilisation du SystÃ¨me

### 1. Fine-tuning avec Tracking

```bash
# Lancement basique
python finetune_pong.py --episodes 500

# Avec paramÃ¨tres personnalisÃ©s
python finetune_pong.py \
    --episodes 500 \
    --lr 1e-4 \
    --gamma 0.99 \
    --clip 0.2 \
    --plot

# Depuis un modÃ¨le spÃ©cifique
python finetune_pong.py \
    --model ppo_pong_20251021_110351/best_model.pth \
    --episodes 500
```

**Sorties automatiques:**
- Dossier `ppo_pong_finetuned_YYYYMMDD_HHMMSS/`
- Fichiers JSON avec toutes les mÃ©triques
- Logs TensorBoard en temps rÃ©el
- Checkpoints rÃ©guliers

### 2. Visualisation TensorBoard

```bash
# Lancer TensorBoard
tensorboard --logdir=ppo_pong_finetuned_20251021_154622/tensorboard

# Puis ouvrir dans le navigateur
# http://localhost:6006
```

**MÃ©triques disponibles:**

| CatÃ©gorie | MÃ©trique | Description |
|-----------|----------|-------------|
| Episode | Reward | RÃ©compense par Ã©pisode |
| Episode | Length | Longueur en frames |
| Episode | Global_Step | Step cumulatif |
| Loss | Total | Somme des losses |
| Loss | Policy | Loss de la politique |
| Loss | Value | Loss du critique |
| Loss | Entropy | Entropie (exploration) |
| PPO | Clip_Fraction | % actions clippÃ©es |

### 3. Analyse Comparative

```bash
# Comparer tous les modÃ¨les
python analyze_performance.py --plot --report

# Comparer des modÃ¨les spÃ©cifiques
python analyze_performance.py \
    --models ppo_pong_A ppo_pong_B ppo_pong_C \
    --plot \
    --report \
    --output comparison.png
```

**Sorties:**
1. **Tableau comparatif** dans le terminal
2. **Graphiques de comparaison** (4 plots):
   - RÃ©compenses lissÃ©es au fil du temps
   - Longueurs d'Ã©pisodes
   - Performance finale (bar chart)
   - Distribution des rÃ©compenses (boxplot)
3. **Rapport dÃ©taillÃ©** en texte (`performance_report.txt`)

### 4. AccÃ¨s Programmatique

```python
import json

# Charger les mÃ©triques
with open('ppo_pong_finetuned_XXXX/metrics.json', 'r') as f:
    metrics = json.load(f)

# AccÃ©der aux Ã©pisodes
episodes = metrics['episodes']
rewards = [ep['reward'] for ep in episodes]

# Statistiques
avg_reward = np.mean(rewards[-100:])  # Moyenne des 100 derniers
print(f"Performance finale: {avg_reward:.2f}")

# AccÃ©der aux updates
updates = metrics['updates']
policy_losses = [u['policy_loss'] for u in updates]
```

## ğŸ“Š InterprÃ©tation des MÃ©triques

### RÃ©compense (Reward)
- **Plage:** -21 Ã  +21 (Pong)
- **Positif:** L'agent gagne plus de points
- **NÃ©gatif:** L'adversaire gagne plus de points
- **Objectif:** Maximiser la rÃ©compense moyenne

### Longueur d'Ã‰pisode
- **Plage:** Variable (500-3000+ frames typique)
- **Court:** Parties rapides (bonne/mauvaise perf)
- **Long:** Ã‰changes Ã©quilibrÃ©s
- **Tendance:** Devrait se stabiliser

### Policy Loss
- **RÃ´le:** Mesure l'amÃ©lioration de la politique
- **Tendance:** Devrait dÃ©croÃ®tre puis se stabiliser
- **Alerte:** Si augmente fortement â†’ divergence

### Value Loss
- **RÃ´le:** PrÃ©cision de l'estimation de valeur
- **Tendance:** Devrait dÃ©croÃ®tre
- **Alerte:** Si reste Ã©levÃ© â†’ critique mal calibrÃ©

### Entropie
- **RÃ´le:** Mesure d'exploration
- **Ã‰levÃ©:** Exploration (bon au dÃ©but)
- **Faible:** Exploitation (normal en fin)
- **Alerte:** Si tombe trop vite â†’ sous-exploration

### Clip Fraction
- **RÃ´le:** % d'actions clippÃ©es par PPO
- **Plage:** 0.0 Ã  1.0
- **Optimal:** 0.1 - 0.3
- **Trop haut (>0.5):** Updates trop agressifs
- **Trop bas (<0.05):** Updates trop conservateurs

## ğŸ¯ ScÃ©narios d'Analyse

### ScÃ©nario 1: Fine-tuning RÃ©ussi

**Signes:**
- âœ… RÃ©compense moyenne augmente
- âœ… Variance se rÃ©duit progressivement
- âœ… Losses dÃ©croissent puis se stabilisent
- âœ… Clip fraction ~ 0.1-0.3
- âœ… Entropie dÃ©croÃ®t lentement

**Actions:**
- Continuer l'entraÃ®nement
- Sauvegarder le modÃ¨le actuel

### ScÃ©nario 2: Stagnation

**Signes:**
- âš ï¸ RÃ©compense plateau depuis 100+ Ã©pisodes
- âš ï¸ Losses stables mais perf ne s'amÃ©liore pas
- âš ï¸ Clip fraction trÃ¨s faible (<0.05)

**Actions:**
- Augmenter le learning rate
- RÃ©duire clip_epsilon
- Augmenter l'entropie bonus (c2)

### ScÃ©nario 3: InstabilitÃ©

**Signes:**
- âŒ RÃ©compense oscille fortement
- âŒ Losses augmentent
- âŒ Clip fraction trÃ¨s Ã©levÃ©e (>0.5)

**Actions:**
- RÃ©duire le learning rate
- Augmenter clip_epsilon
- RÃ©duire la batch size
- Repartir d'un checkpoint antÃ©rieur

### ScÃ©nario 4: Overfitting

**Signes:**
- âš ï¸ Losses continuent de baisser
- âš ï¸ Performance test se dÃ©grade
- âš ï¸ Entropie proche de 0

**Actions:**
- ArrÃªter l'entraÃ®nement
- Utiliser un checkpoint antÃ©rieur
- Augmenter c2 (entropy bonus)

## ğŸ“ Structure des Fichiers

```
ppo_pong_finetuned_20251021_154622/
â”œâ”€â”€ metrics.json              # MÃ©triques complÃ¨tes
â”œâ”€â”€ config.json               # Configuration d'entraÃ®nement
â”œâ”€â”€ best_model.pth           # Meilleur modÃ¨le (avg 100 derniers)
â”œâ”€â”€ checkpoint_ep50.pth      # Checkpoint Ã©pisode 50
â”œâ”€â”€ checkpoint_ep100.pth     # Checkpoint Ã©pisode 100
â”œâ”€â”€ ...
â”œâ”€â”€ finetuning_results.png   # Graphiques (si --plot)
â””â”€â”€ tensorboard/             # Logs TensorBoard
    â”œâ”€â”€ events.out.tfevents.xxx
    â””â”€â”€ ...
```

## ğŸ”§ Configuration AvancÃ©e

### Modifier la frÃ©quence de sauvegarde

Dans `finetune_pong.py`:
```python
save_every=50    # Sauvegarder tous les 50 Ã©pisodes
log_every=10     # Logger tous les 10 Ã©pisodes
```

### Ajouter des mÃ©triques personnalisÃ©es

```python
# Dans PerformanceTracker.log_episode()
def log_episode(self, episode, reward, length, global_step, custom_metric=None):
    metric = {
        'episode': episode,
        'reward': reward,
        'length': length,
        'global_step': global_step,
        'custom': custom_metric,  # Nouvelle mÃ©trique
        'timestamp': datetime.now().isoformat()
    }
    self.episode_metrics.append(metric)
    
    if self.writer:
        self.writer.add_scalar('Custom/Metric', custom_metric, episode)
```

### Exporter vers d'autres formats

```python
import pandas as pd

# Charger les mÃ©triques
with open('metrics.json', 'r') as f:
    metrics = json.load(f)

# Convertir en DataFrame
df_episodes = pd.DataFrame(metrics['episodes'])
df_updates = pd.DataFrame(metrics['updates'])

# Exporter en CSV
df_episodes.to_csv('episodes.csv', index=False)
df_updates.to_csv('updates.csv', index=False)

# Exporter en Excel
with pd.ExcelWriter('metrics.xlsx') as writer:
    df_episodes.to_excel(writer, sheet_name='Episodes', index=False)
    df_updates.to_excel(writer, sheet_name='Updates', index=False)
```

## ğŸ“Š Workflow RecommandÃ©

### 1. Lancement
```bash
python finetune_pong.py --episodes 500 --lr 1e-4
```

### 2. Monitoring en temps rÃ©el
```bash
# Terminal 2
tensorboard --logdir=ppo_pong_finetuned_*/tensorboard
```

### 3. Analyse pÃ©riodique
```bash
# Toutes les 100 Ã©pisodes, vÃ©rifier:
python analyze_performance.py --plot
```

### 4. Ajustements
- Si stagnation â†’ modifier hyperparamÃ¨tres
- Si bon progrÃ¨s â†’ continuer
- Si divergence â†’ rollback checkpoint

### 5. Validation finale
```bash
# Tester le meilleur modÃ¨le
python play_pong.py --games 20

# Analyse comparative
python analyze_performance.py --plot --report
```

## ğŸ“ Conseils d'Optimisation

### Pour accÃ©lÃ©rer l'entraÃ®nement
- Augmenter batch_size (256 â†’ 512)
- RÃ©duire update_every (2048 â†’ 1024)
- Utiliser GPU si disponible

### Pour amÃ©liorer la stabilitÃ©
- RÃ©duire learning_rate (1e-4 â†’ 5e-5)
- Augmenter clip_epsilon (0.2 â†’ 0.3)
- Augmenter epochs (4 â†’ 6)

### Pour favoriser l'exploration
- Augmenter c2 entropy bonus (0.01 â†’ 0.02)
- DÃ©marrer avec learning_rate plus Ã©levÃ©

## ğŸ” Debugging

### Metrics non sauvegardÃ©s
```python
# VÃ©rifier que tracker.save_metrics() est appelÃ©
tracker.save_metrics()

# Forcer la sauvegarde pÃ©riodique
if (episode + 1) % 50 == 0:
    tracker.save_metrics()
```

### TensorBoard ne montre rien
```bash
# VÃ©rifier le chemin
ls ppo_pong_finetuned_*/tensorboard/

# Forcer le flush
if self.writer:
    self.writer.flush()
```

### MÃ©triques incohÃ©rentes
```python
# Ajouter des assertions
assert -21 <= reward <= 21, f"Reward hors limites: {reward}"
assert length > 0, f"Longueur invalide: {length}"
```

## ğŸ“š Ressources

- **TensorBoard Guide**: https://www.tensorflow.org/tensorboard
- **PPO Paper**: https://arxiv.org/abs/1707.06347
- **Gymnasium Docs**: https://gymnasium.farama.org/
- **PyTorch Docs**: https://pytorch.org/docs/

## âœ… Checklist

Avant chaque session d'entraÃ®nement :
- [ ] VÃ©rifier que le bon modÃ¨le de base est chargÃ©
- [ ] Confirmer les hyperparamÃ¨tres
- [ ] VÃ©rifier l'espace disque disponible
- [ ] Lancer TensorBoard pour monitoring
- [ ] DÃ©finir un critÃ¨re d'arrÃªt clair

AprÃ¨s chaque session :
- [ ] Sauvegarder les mÃ©triques (`tracker.save_metrics()`)
- [ ] GÃ©nÃ©rer les graphiques de comparaison
- [ ] Documenter les observations
- [ ] Backup des meilleurs modÃ¨les
- [ ] Commit sur Git avec description

---

**DerniÃ¨re mise Ã  jour:** 21 octobre 2025
